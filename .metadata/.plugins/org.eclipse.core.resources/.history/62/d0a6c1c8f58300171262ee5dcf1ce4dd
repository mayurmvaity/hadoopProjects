import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;


public class TokenizerProPgm {

	public static class TMapper extends Mapper<LongWritable, Text, Text,IntWritable>
	{
		public void map(LongWritable key, Text value, Context context)
		{
			try
			{
				
			}
			catch(Exception r)
			{
				System.out.println(r.getMessage());
			}
		
			Text word = new Text();
			IntWritable one = new IntWritable(1);
			
			StringTokenizer str = new StringTokenizer(value.toString());
			
			while(str.hasMoreTokens())
			{
				String myWord = str.nextToken().toLowerCase();
				
				word.set(myWord);
			}
			
			context.write(word, one);
		}
	}
	
}
