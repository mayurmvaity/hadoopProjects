
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;


public class HighTranxPgm {

	// Mapper class
	
	public static class HTMapper extends Mapper<LongWritable, Text, Text, Text>
	{
		public void map(LongWritable key, Text value, Context context)
		{
			try {
				String[] str = value.toString().split(";");
				String mykey = "all";
				String dt = str[0];
				String custid = str[1];
				String sales = str[8];
				String myValue = dt + ',' + custid + ',' + sales;
				context.write(new Text(mykey), new Text(myValue));
			}
			catch(Exception e)
			{
				System.out.println(e.getMessage());
			}
		}
	}
	
	// Reducer class
	
	public static class HTReducer extends Reducer<Text, Text, NullWritable, Text>
	{
		long maxvalue = 0;
		String custid = "";
		String dt = "";
		
		for(Text val : values)
		{
			String[] token = val.toString().split(",");
			if(Long.parseLong(token[2]) > maxvalue)
			{
				maxvalue = Long.parseLong(token[2]);
				dt = token[0];
				custid = token[1];
			}
		}
		String myMaxValue = String.format("%d", maxvalue);
		String myValue = dt+','+custid+','+myMaxValue;
		
		
	}
	
}
